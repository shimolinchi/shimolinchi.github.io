---
layout:     post
title:      
subtitle:   
date:       2025-
author:     试墨临池
header-img: img/2025-01-26-机器学习/background.jpg
catalog: true
tags:
    - 
---

## 多臂赌博机问题

评估是相对指令而言的。教导式学习的反馈通常是给定什么指令是正确的，而评估式学习的反馈是给定行为的得分。

定义$q_*(a)$是给定选择a的预期奖励。则：

$$q_*(a)\dot =E[R_t\|A_t = a]$$

用$Q_t(a)$表示在t回合时动作a的估计奖励。我们希望$Q_t(a)\rightarrow q_*(a)$。若在t回合之前行为a被选了$k_\alpha$次，产生了$r_1,...,r_{k_\alpha}$的汇报，则估计行为值为：

$$Q_t(a) \dot= \frac{r_1+...+r_{k_\alpha}}{k_\alpha}$$

$k_\alpha = 0$时，令其默认值$Q_0(a) = 0$,当$k_\alpha$无限大时，由大数定律，$Q_t(a)$收敛到$Q^*(a)$

下面时两种选择规则：

+ **贪婪算法**：每回合都选最高行为值的行为
+ **$\varepsilon-greedy$算法**：大部分回合以贪婪算法，但以小概率$\varepsilon$选择一个均匀的、独立于行为估计值的随即行为。

下图是原书在概念上区别的通俗解释。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-机器学习/20.png)

对于行为值方法，若回合无限多，则需要的内存也是无限多。因此采用如下的 **增量式** 实现：

$$Q_{n+1} \dot= \frac{1}{n}\underset{i=1}{\overset{n}{\Sigma}}R_i = Q_n + \frac{1}{n}[R_n-Q_n]$$

对于如上公式，采用如下演化，以适应一个非稳态的情况。

$$Q_{n+1} = Q_n + \alpha[R_n-Q_n]，\ 0<\alpha\leq1 $$

这里将$\frac{1}{n}$变成了$\alpha$，导致不同时期奖励的权重发生变化，即权重根据$1-\alpha$的指数呈指数衰减。

书上提到的步长就是$\alpha_n(a)$参数。随机逼近理论提供保证收敛的条件为：

$$\begin{align*}
    & \underset{n=1}{\overset{\infty}{\Sigma}}\alpha_n(a) = \infty \ 和

    & \underset{n=1}{\overset{\infty}{\Sigma}}\alpha_n^2(a) < \infty&& 
\end{align*}$$

下面介绍另一种动作选择算法：**上置信区间 UCB**

$$A_t \dot= \underset{a}\argmax\left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right]$$

+ argmax是当[]的内容取最大值时对应自变量a的值
+ $N_t(a)$表示在时间t之前选择动作a的次数
+ c 表示控制探索的程度（说是通常设置为2）
+ 平方根项是对一个值估计的不确定性或方差的度量
+ 总体思想就是后面那一项与这个动作被探索的程度（做过的次数）有关，鼓励开发一些没被探索的动作

说是这个算法在k臂赌博机效果比 $\varepsilon-$贪婪算法 更好一点，但是难以处理非平稳问题和大状态空间时的问题

```py
class UCB:
    def __init__(self, n_arms, c=2):
        self.n_arms = n_arms
        self.c = c
        self.Q = np.zeros(n_arms)  # 动作的估计收益
        self.N = np.zeros(n_arms)  # 动作的选择次数
        self.t = 0  # 总步数

    def select_action(self):
        # 计算 UCB 值
        ucb_values = self.Q + self.c * np.sqrt(np.log(self.t + 1) / (self.N + 1e-5))
        return np.argmax(ucb_values)

    def update(self, action, reward):
        # 更新动作的选择次数和估计收益
        self.N[action] += 1
        self.Q[action] += (reward - self.Q[action]) / self.N[action]
        self.t += 1
```

到此对每个特定的动作没有特定的偏好，下面就要对动作加上 **偏好** ，表示为$H_t(a)$。需要解释的是：

+ 偏好是相对的
+ 偏好与奖励无关

在偏好影响下，**动作概率** 遵循softmax分布：

$$Pr\{A_t = a\} \dot= \frac{e^{H_t(a)}}{\sum^k_{b=1}e^{H_t(b)}} = \pi_t(a)$$

$\pi(a)$是选择动作a的概率。通过动作概率的方式，可以平衡了动作探索和回报更大两者之间的关系

```py
def softmax_prob_choose(H)
    exp_H = np.exp(H - np.max(H))
    action_probs = exp_H / np.sum(exp_H)
    return np.random.chioce(len(H), p=action_probs)
```

动作偏好更新方式如下：

$$H_{t+1}(A_t) \dot= H_t(A_t) + \alpha(R_t - \bar R_t)(1-\pi_t(A_t))$$

$$ H_{t+1}(a) \dot= H_t(a)-\alpha(R_t - \bar R_t)\pi_t(a)\ , \ for\ all\ a\ \neq\ A_t$$

+ $\alpha$：步长参数
+ $\bar R_t$：截至回合t的所有奖励的平均值（包含回合t）。称为 **奖励基线 Reward Baseline**

原文这里说如果去掉了奖励基线（即将其置为0），算法表现大大降低。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-机器学习/21.png)

到现在为止只考虑了非关联性任务，还有需要将不同行为和不同情况联系起来的任务，如关联搜索任务。

下图是以上提到的几种动作选择算法的比较图：

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-机器学习/22.png)

## 有限马尔科夫决策过程  Finite Markov Decision Processes

在赌博机问题中，我们估计每个动作的价值$q_*(a)$，但在 MDP 中，我们估计关于每个状态中每个动作的价值$q_*(s,a)$。

MDP中的学习者和决策者被称为 **个体 Agent**，其与之交互的东西被称为 **环境 environment**

![MDP中的个体-环境交互](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-机器学习/23.png)

对于每个回合t，个体接受环境状态$S_t$，做出动作$A_t$，收到奖励$R_{t+1}$，达到新状态$S_{t+1}$。

对于给定前一状态和动作的情况下，存在下一状态和回报的概率：

$$p(s^′,r|s,a)\dot=Pr\{S_t=s^′,R_t=r|S_{t−1}=s,A_{t−1}=a\}$$

在MDP中，$S_t$和$R_t$的概率仅取决于前一个状态和动作，不依赖于先前的状态和动作的话（状态必须包括有关过去的个体-环境交互的所有方面的信息），就说该状态具有 **马尔可夫性 Markov property**