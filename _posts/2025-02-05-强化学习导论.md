---
layout:     post
title:      
subtitle:   
date:       2025-
author:     试墨临池
header-img: img/2025-01-26-机器学习/background.jpg
catalog: true
tags:
    - 
---

## 评估式反馈

评估是相对指令而言的。教导式学习的反馈通常是给定什么指令是正确的，而评估式学习的反馈是给定行为的得分。

定义$q_*(a)$是给定选择a的预期奖励。则：

$$q_*(a)\dot =E[R_t\|A_t = a]$$

用$Q_t(a)$表示在t回合时动作a的估计奖励。我们希望$Q_t(a)\rightarrow q_*(a)$。若在t回合之前行为a被选了$k_\alpha$次，产生了$r_1,...,r_{k_\alpha}$的汇报，则估计行为值为：

$$Q_t(a) = \frac{r_1+...+r_{k_\alpha}}{k_\alpha}$$

$k_\alpha = 0$时，令其默认值$Q_0(a) = 0$,当$k_\alpha$无限大时，由大数定律，$Q_t(a)$收敛到$Q^*(a)$

下面时两种选择规则：

+ **贪婪算法**：每回合都选最高行为值的行为
+ **$\varepsilon-greedy$算法**：大部分回合以贪婪算法，但以小概率$\varepsilon$选择一个均匀的、独立于行为估计值的随即行为。

下图是原书在概念上区别的通俗解释。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-机器学习/20.png)

对于行为值方法，若回合无限多，则需要的内存也是无限多。因此采用如下的 **增量式** 实现：

$$Q_{n+1} = \frac{1}{n}\underset{i=1}{\overset{n}{\Sigma}}R_i = Q_n + \frac{1}{n}[R_n-Q_n]$$

对于如上公式，采用如下演化，以适应一个非稳态的情况。

$$Q_{n+1} = Q_n + \alpha[R_n-Q_n]，\ 0<\alpha\leq1 $$

这里将$\frac{1}{n}$变成了$\alpha$，导致不同时期奖励的权重发生变化，即权重根据$1-\alpha$的指数呈指数衰减。

书上提到的步长就是$\alpha_n(a)$参数。随机逼近理论提供保证收敛的条件为：

$$\begin{align*}
    & \underset{n=1}{\overset{\infty}{\Sigma}}\alpha_n(a) = \infty \ 和

    & \underset{n=1}{\overset{\infty}{\Sigma}}\alpha_n^2(a) < \infty&& 
\end{align*}$$

下面介绍另一种动作选择算法：**上置信区间 UCB**

$$A_t = \underset{a}\argmax\left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right]$$

+ argmax是当[]的内容取最大值时对应自变量a的值
+ $N_t(a)$表示在时间t之前选择动作a的次数
+ c 表示控制探索的程度（说是通常设置为2）
+ 平方根项是对一个值估计的不确定性或方差的度量
+ 总体思想就是后面那一项与这个动作被探索的程度（做过的次数）有关，鼓励开发一些没被探索的动作

说是这个算法在k臂赌博机效果比 $\varepsilon-$贪婪算法 更好一点，但是难以处理非平稳问题和大状态空间时的问题