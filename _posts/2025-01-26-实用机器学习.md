---
layout:     post
title:      实用机器学习
subtitle:   
date:       2025-1-26
author:     试墨临池
header-img: img/2025-01-26-实用机器学习/background.jpg
catalog: true
tags:
    - 机器学习
    - ML
    - 笔记
---

## 数据

### 数据获取

#### 数据集来源

+ MNIST:手写数据集
+ ImageNet:图片数据集
+ AudioSet:youtube上的声音切片
+ Kinetics:youtube上的人行为视频切片
+ KITTI:无人驾驶中传感器的数据集
+ Amazon Review:亚马逊的用户评论
+ SQuAD:维基百科上的问答对
+ LibriSpeech:1000小时的有声读物

#### 数据集网站

+ [paperswithcode](https://paperswithcode.com/datasets):学术常用数据集
+ [kaggle](https://www.kaggle.com/datasets):数据科学家上传的机器学习数据集
+ [google dataset search](https://datasetsearch.research.google.com/):谷歌搜索数据集
+ [tensorflow](https://www.tensorflow.org/datasets?hl=zh-cn):工具包数据集
+ [huggingface](https://huggingface.co/docs/datasets/index):专注于做文本的transeformer数据库
+ [Open Data on AWS](https://registry.opendata.aws/):大规模原数据

#### 生成合成数据

+ GANS（生成式对抗网络）：
  + [生成人脸](https://thispersondoesnotexist.com/)
+ 仿真
+ 数据增强：
  + [来回翻译](https//amitness.com)
  + [图像增强](https://github.com/aleju/imgaug)

#### 爬网页数据

### 数据标注

### 数据分析

### 数据清理

### 数据变换

### 特征工程

## 机器学习简单算法

### 机器学习介绍

机器学习分为：

+ 有监督学习 Supervised：数据有标号，模型来预测标号
+ 半监督学习 Semi-supervised:有一些数据有标号，但也要使用无标号的数据
+ 无监督学习：数据没有标号，模型的任务也不是去标号
+ 强化学习：利用与环境交互的观察结果采取行动而获得最大化的奖励

**损失函数 Loss**：模型预测值和真实值的差距的一类型函数

**目标函数 Objective**：将损失函数最小时的模型各参数

**模型优化 Optimization**：解出目标函数的算法

有监督机器学习的模型分类：

+ 决策树 Decision trees
+ 线性模型 Linear methods
+ 核方法 Kernel machines:使用核函数来衡量两个样本的相似度
+ 神经网络 Neural Networks:用神经网络来学习特征的表示

### 决策树

![决策树 & 生成树](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/1.png)

**优点**：

1. 可以被解释
2. 可以处理数值类或类别类的特征

**缺点**：

1. 不稳定（易受噪音影响）
2. 导致过拟合（树枝太多了）
3. 不好在并行计算的GPU上进行部署（性能上吃亏）

**缺点的解决方法**：

1. 随机森林 Random Forest（解决不稳定）：独立训练多棵树，将结果投票或取。随机指的是在每一棵树的训练过程中，数据是从数据集中随机选取的，可能重复选中也可能选不到。而且树的特征选取也是随机的
2. 梯度上升决策树 GBDT

初始化：首先，GBDT会用一个简单的模型（如常数模型）对所有样本做出初始预测。
迭代过程：
   + 计算残差：基于当前模型的预测结果，计算每个样本的真实标签与预测值之间的梯度（对于回归问题通常是真实值减去预测值；对于分类问题，则使用损失函数的负梯度）。
   + 拟合决策树：将这些残差作为新的目标变量，训练一个决策树来拟合这些残差。决策树的深度和节点数决定了模型的复杂度。
   + 更新预测：将新训练的决策树加入到模型中，更新每个样本的预测值为原预测值加上新决策树的输出。

### 线性模型

对于预测问题，当数据的特征$x_1 x_2 ...x_n$，线性模型的预测结果为$y = w_1x_1+w_2x_2+...+w_nx_n+b$

目标就是优化平均均方误差 MSE

$$J(w,b) = \frac{1}{n}\underset{i=1}{\overset{n}{\Sigma}}(y_i-<x_i,w>-b)^2$$

参数获取的过程（随机梯度下降 SGD）：

首先随机初始化$w_1$

然后将t从1，2，...按如下进行迭代，直到参数收敛：

随机采取样本$I_t\subset\{1,...,n\}$且$\|I_t\| = b$（这里b是每次选取样本数量大小，每次使用的小批量 mini-batch）

更新$w_{t+1} = w_t-\eta_t\nabla_{w_t}l(X_{I_t},y_{I_t},w_t)$

即：

$$w_{t+1} = w_t - \eta \frac{\partial J(w_t, b_t)}{\partial w}$$

$$b_{t+1} = b_t - \eta \frac{\partial J(w_t, b_t)}{\partial b}$$

其中，$\eta$为学习率，J为损失函数

对于分类问题，输出的就是一个类别。假设实际类别为 $[y_1,y_2...y_m] (y_i = 1\ if\ i=y\ otherwise\ 0)$计算方法为先计算各个类别的置信度$o_i = <x,w_i>+b_i$（用预测的方法计算）然后通过如下方法进行分类：

+ **softmax算子** 来将置信度转化为方便决策的 **概率** 形式：

$$\hat y = softmax(o)\ where\ \hat y_i = \frac{exp(o_i)}{\underset{k=1}{\overset{m}{\Sigma}}exp(o_k)}$$

衡量正确预测的概率：

$$H(y,\hat y) = \underset{i}{\overset{}{\Sigma}}-y_ilog(\hat y_i) = -log\hat y_y$$

## 神经网络

神经网络一定意义上可以说是代替人进行的数据特征提取

### 多层感知机 MLP

多层感知机中最重要的概念是 **全连接层 dense layer** 。在该层中有可以学习的参数$W\in R^{m\times n},b\in R^m$。输入通过全连接层将输出结果$y = Wx+b\in R^m$

这里可以借助线性模型理解：线性模型就是输入维度为1（即一个向量）通过全连接层的结果。将其扩展成含有m个特征的n个数据可输出n个结果组成的向量

如上的模型也只停留在线性阶段。要引入非线性，需使用 **激活函数** ：

$$sigmoid(x) = \frac{1}{1+exp(-x)},\ \ \tanh x = \frac{e^x-e^{-x}}{e^x+e^{-x}},\ \ ReLu(x) = max(x,0)（有点像二极管）$$

通过全连接层和激活层的交替，就实现了多层感知机算法

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/2.png){: width="60%" height="60%"}

与输出不相连的全连接层就是 **隐藏层** ,最后一个全连接层叫做 **分类层**

来个超简单经典的示例代码

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# 1. 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data  # 特征
y = iris.target  # 标签

# 2. 分割数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. 创建 MLP 分类器
# hidden_layer_sizes 参数指定隐藏层的结构，这里是一个含有 10 个神经元的隐藏层
mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, activation='relu', solver='adam', random_state=42)

# 4. 训练 MLP 模型
mlp.fit(X_train, y_train)

# 5. 预测测试集
y_pred = mlp.predict(X_test)

# 6. 计算并输出准确率
accuracy = accuracy_score(y_test, y_pred)
print(f'模型准确率: {accuracy * 100:.2f}%')

```

### 卷积神经网络 CNN

CNN实际上是从MLP中的全连接层过渡到 **卷积层** 。卷积的产生：由于当输入和输出的特征特别多时（如图片，具有很多很多像素），需要学习的参数太多（全连接层输出和输入的乘积）。而在这样的应用中通常具有如下两个性质：

+ 平移不变性：如图像类别识别中，要识别的特征位置发生平移也一定能识别到

+ 本地性：离得近的像素相关性就高，不需要找离得远的像素

卷积就是利用了这两个性质。通过 **卷积核 kernel** 与对应的像素进行卷积，替代了原本庞大的权重矩阵与输入进行乘积，这样可学习参数个数只与卷积核大小k相关了，减小了学习成本。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/3.png){: width="60%" height="60%"}

单核卷积神经网络的实现方式：

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/4.png){: width="60%" height="60%"}

在卷积计算之后，输出的大小仍然没有改变，在这样的一张记录所有输出的矩阵图中，存在着很多无用信息（即经过卷积计算后仍然较小的数，这代表着与特征的相关性不高），因此需要一个新的层来将无用信息删除掉。这样就诞生了 **池化层 pooling layer**。其作用主要是降维、去除冗余信息、减少下一层的计算量等。

池化分为 **最大池化 max pooling**和 **平均池化 average pooling**。下图展示的是最大池化，平均池化就望文生义了。此外还有重叠池化 overlapping pooling（相邻窗口之间有重叠区域）、空金字塔池化 Spatial Pyramid pooling（不细说）

![最大池化](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/5.png){: width="60%" height="60%"}

卷积神经网络 CNN 就是将卷积层、激活层、池化层一层一层堆叠起来，最后连着一个分类层获得输出的算法。

### 循环神经网络 RNN

对于自然语言模型，通常是进行预测，即给定一个单词和语境，预测下一个单词。若使用MLP算法，输入的维度是不能发生变化的，即我只能依靠当前的单词来预测下一个单词，不能结合之前的语境。因此RNN算法就是解决非固定长度输入的。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/6.png){: width="60%" height="60%"}

这里RNN模型经过第一个全连接层后的结果包含了输入的信息，在经过Split运算后与下一个循环进行Concat运算，这样就实现了最开始输入信息的传递（hello）。下面是一个最简单的RNN：

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/7.png){: width="60%" height="60%"}

这里每一层循环都经过两个输入：一个当前时刻输入$x_t$，一个包含上一时刻信息的隐藏态$h_{t-1}$。在当前时刻的输出$y_t$复制一份以隐藏态$h_t$。这里$y_t$和$h_t$本质上是一个东西（在此简单版本中）。

复杂一点的RNN含有 **门 Gated RNN**，如**LSTM、GRU**。会选择性抑制一些输入，如语言模型中对于介词等词汇会选择忘掉$x_t$，当新的一段开始时会忘掉前一段一些信息$h_{t-1}$，这些忘掉信息的参数也是通过学习的。

为了适应句子方向的左右需求，即有时需要理解整个句子的语义，产生了双向RNN Bi-RNN。双向RNN存在着反向时时刻t+1的输出作为隐藏状态作用于时刻t的全连接层。这样的算法可以更好适应整个语境的信息，如做完形填空的时候。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/8.png){: width="60%" height="60%"}

#### Seq2seq 

此模型用于处理序列到序列的问题，例如文本翻译等。

Seq2seq是一个 **编码 Encode** **解码 Decode** 架构。编码器是一个RNN，读取输入句子（可以是双向），解码器是另一个RNN用于输出。编码器将最后的隐藏状态（包含句子所有的信息）传给解码器

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/9.png){: width="60%" height="60%"}

解码器在训练和推理的时候输入不同。在训练的时候，输入的都是正确的目标句子，而在推理的时候的输入是上一循环推理出的结果。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/10.png){: width="60%" height="60%"}

seq2seq中层的表示如下：

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/11.png){: width="60%" height="60%"}

为衡量生成序列好坏，采用了 **BLEU**，定义如下：

$$exp(min(0,1-\frac{len_{label}}{len_{pred}}))\underset{n=1}{\overset{k}{\Pi}}p_n^{\frac{1}{2^n}}$$

其中，p_n是预测中所有结果的精度。假如预测结果是ABBCD，正确结果是ABCDEF，则$p_1 = 4/5,p_2 = 3/4,p_3 = 1/3,p_4 = 0$,n代表预测结果中连续的n个单元，$p_n$代表匹配的数量占总量的比例

BLEU最大值为1，此时为完美预测。这里前一段的指数项用于惩罚过短的预测，而后一段表现为长的匹配具有高权重（即如果长匹配过少，就狠狠惩罚）

#### 引入注意力机制的 seq2seq

在上述的seq2seq中，编码器输出的隐藏层进入解码器的输入，这一隐藏层包含的是整个句子的整体信息，但是对于特定字节（即循环到的特定位置），理论上应对应于一些原始字符，这应当被注意（举例：英文翻译时一些英文单词对应中文的一些词语更加合理）。原本的seq2seq无法对此直接建模，因此便引入了 **注意力机制**。不说人话的话就是原本的模型存在信息瓶颈（上下文易丢失细节）和难以捕捉长距离依赖问题

实现方式如下

对于解码器的输入，除了原本的嵌入层 Embeding 和编码器输出的隐藏态（二者concat一起）之外，增加了一些。

+ 将编码器对每次词的输入作为key和value（key和value是等价的一对）放进Attention里面
+ 解码器RNN对上一词的输出是query也放进Attention里
+ Attention的输出和下一个词经过嵌入层的结果合并（concat）进入下一个循环的解码器RNN

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/11.png)

这里键就是编码器的隐藏状态，姑且先认为键和值是一个东西。而查询就是解码器的隐藏状态。这里用形象的比喻：

查询Query问注意力层：“我现在需要生成一个词，我这里有输出上一个词包含语义的特征”

键Key告诉注意力层：“截至目前，输入序列的每个部分都有这些信息，你可以根据需要选择。”

注意力层告诉解码器：“比较查询和键给我的输入，他们的相似度打分经过softmax归一化后,我将这个结果和值Value进行加权求和了，给你这个生成的上下文向量”

### 变换神经网络 Transformer

#### 自注意力

自注意力池化层是将给定序列$x_1,x_2,...,x_n$当作key、value、query来对序列抽取特征得到$y_1,y_2,...,y_n$。这里：

$$y_i = f(x_i,(x_1,x_1),...,(x_n,x_n))$$

下面是三种模型的对比（n是序列长度，d是一个token特征向量的长度，k是卷积核大小）

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/13.png){: width="60%" height="60%"} 

由于自注意力机制没有记录位置信息（与CNN、RNN不同），如果没有位置信息输入的语序都是乱的也能训练出来模型输出肯定在胡言乱语。因此引入一个位置编码将位置信息灌入输入中。假设输入序列长度为n，$X\in R^{n\times d}$，那么位置编码矩阵为$P\in R^{n\times d}$，输入就改为$X+P$(相加这个挺玄学的反正)。P中的元素计算如下：

$$\begin{align*}
    & p_{i,2j} = \sin(\frac{i}{10000^{2j/d}})
    & p_{i,2j+1} = \cos(\frac{i}{10000^{2j/d}})  
    && 
\end{align*}$$

至于为什么要这样设计，是因为这样两个词之间通过线性变换投影后相对位置不变。我们通常只关注句子中词的相对位置，不关注绝对位置，因此这样是合理的。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/14.png){: width="50%" height="50%"}

#### Transformer 架构

Transformer仍然使用编码器-解码器架构和注意力机制，但与seq2seq不同的是，Transformer是纯基于自注意力的架构（没有RNN）。

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/15.png){: width="60%" height="60%"}

图中的 加&规范化 就是一个全连接层。这里面左边虚线框里的东西叫做一个Transformer块。

所谓的**多头注意力**是指：对于同一组Key Value Query，采用多通道的注意力运算后用concat运算并起来。有点类似于卷积神经网络中的多通道识别

![](https://raw.githubusercontent.com/shimolinchi/shimolinchi.github.io/master/img/2025-01-26-实用机器学习/16.png){: width="60%" height="60%"}

看一下多头注意力可学习参数的维度

对于$q\in R^{d_q}, \ k\in R^{d_k},\ v\in R^{d_v}$

头i可学习的参数$W_i^{(q)}\in R^{p_q\times d_q},W_i^{(k)}\in R^{p_k\times d_k},W_i^{(v)}\in R^{p_v\times d_v}$

头i的输出$h_i = f(W_i^{(q)}q,W_i^{(k)}k,W_i^{(v)}v)$

输出的可学习参数$W_o\in R^{o_o\times hp_v}$

多头注意力的输出为

$$W_o\left(
\begin{matrix}
h_1\\
\dots \\
h_h
\end{matrix}
\right)\in R^{p_o}$$

**有掩码的多头注意力**是指：解码器 对序列中一个元素输出时，不应该考虑之后的元素，即计算$x_i$时，假装当前序列长度为i，将之后的忽略

所谓的 **基于位置的前馈网络 FFN** 是指：对于多头自注意力层的输出向量$X=(x_1,...,x_n)$对每一个$x_i$都进行一次全连接层计算。由于每一个$x_i$代表着位置不同的词，因此称为*基于位置*。输入时形状由$(b,n,d)$变换成$(bn,d)$（第一个全连接层），然后激活，输出时形状由$(bn,d)$变换回$(b,n,d)（第二个全连接层）$。这样的作法就是因为将位置有关的变量b和n（b是批量大小，n是序列长度）整合到一起，方便对每个位置进行非线变。

**层归一化** 听不懂了

信息传递：编码器的输出为$y_1,...,y_n$，将其作为解码器中国第i个Transformer块中多头注意力的key和value（它的query来自目标序列）。这里下面的两个多头注意力都是自注意力，但是上面的多头是普通注意力。一位置编码器和解码器中块的个数和输出维度是一样的。